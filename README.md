# Crawler

## Описание

**Crawler** — это многопоточный map-reduce файловый краулер, разработанный для эффективного обхода и обработки большого количества файлов с использованием worker pool.  
Проект реализует:

- многопоточную обработку файлов;
- корректную работу с отменой контекста;
- эффективную агрегацию результатов;
- работу с небуферизированными каналами;
- модульную архитектуру для переиспользования компонентов.

---

## Основные интерфейсы

### Worker Pool (`Pool`)

- `Transform` — параллельное преобразование данных.
- `Accumulate` — параллельная агрегация результатов.
- `List` — параллельный обход иерархии (например, файлов).

### Краулер (`Crawler`)

Краулер выполняет:

- Рекурсивный обход файловой системы;
- Параллельное чтение и десериализацию файлов;
- Агрегацию результатов с помощью аккумулятора и комбайнера.

---

## Конфигурация

```go
type Configuration struct {
    SearchWorkers      int // Количество воркеров для поиска файлов
    FileWorkers        int // Количество воркеров для обработки файлов
    AccumulatorWorkers int // Количество воркеров для аккумулирования результатов
}
```

## Особенности реализации

- Полная поддержка отмены контекста
- Использование только **небуферизированных каналов**
- Корректное закрытие каналов после обработки
- Обработка ошибок чтения и десериализации файлов
- Поддержка нейтрального элемента и ассоциативности в комбайнере
- Потокобезопасные аккумуляторы и трансформеры

---

## Как запустить

Запустить полный цикл (линтер + тесты):

```bash
make all
```

Запустить только тесты:

```bash
make test
```

Запустить линтер:

```bash
make lint
```

## Структура проекта

- `internal/workerpool/pool.go` — реализация worker pool (Transform, Accumulate, List)
- `internal/filecrawler/crawler.go` — реализация краулера (Collect)
- `internal/fs/filesystem.go` — интерфейс файловой системы
- `tests/` — модульные и интеграционные тесты
